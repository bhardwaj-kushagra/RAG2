# Conversation Q&A – ML/RAG Internals

This document captures and organizes your recent questions (and their answers) about how the RAG system actually works under the hood: embeddings, tokenization, FAISS, models, downloading, and local execution.

It’s written so you can:
- Quickly revisit what you asked and how it was answered.
- Use it as a script when explaining these concepts to others.

---

## 1. How are files ingested using the API (without the web UI)?

### Q: How do I ingest/send local files using APIs, not the React UI?

**Short answer:**
You have two ways:
1. Copy files into `data/docs/` and call `POST /ingest/files`.
2. (Optional) Use an upload endpoint that saves a file then runs ingestion.

**Details:**
- **Method 1 – Scan local folder**
  - Place files (txt, pdf, csv, etc.) in `data/docs/`.
  - Call in PowerShell:
    ```powershell
    Invoke-RestMethod -Uri "http://localhost:8000/ingest/files" -Method POST
    ```
  - The backend scans `data/docs/`, extracts text, chunks it, and inserts passages into MongoDB.

- **Method 2 – Upload (if using `/ingest/upload`)**
  - Send a `multipart/form-data` request with a file.
  - The API saves it into `data/docs/`, then calls the same ingestion logic.

Either way, ingestion always ends up in the same place: text chunks in MongoDB.

---

## 2. What is `faiss.index` for?

### Q: What is this `faiss.index` file and why do we need it?

**Answer:**
`faiss.index` is the on-disk **vector index** for fast similarity search.

- When you build the index, the system:
  1. Reads all passage texts from MongoDB.
  2. Uses the embedding model (`all-MiniLM-L6-v2`) to convert each passage into a 384‑dimensional vector.
  3. Stores these vectors in a FAISS index structure (`IndexFlatIP`).
  4. Saves that index to `faiss.index`.
- At query time, FAISS loads this file and performs nearest‑neighbor search to find the passages most similar to the query embedding.

You should not edit this file manually; it’s binary and regenerated by the code.

---

## 3. How is text converted into vectors (embeddings)?

### Q: Conceptually, how do we go from text like "What is machine learning?" to numbers?

**Answer (high level):**
1. **Tokenization:** Breaks text into tokens (sub‑words) and maps them to integer IDs.
2. **Transformer pass:** A small BERT‑like model (MiniLM) computes a contextual vector for every token.
3. **Pooling:** These token vectors are averaged (mean pooling) into a single sentence vector (384‑D).
4. **Normalization:** The vector is L2‑normalized to length 1 so cosine similarity matches inner product.

**Result:**
- For each text chunk, you get a vector like `[0.02, -0.03, 0.11, ..., -0.01]` of length 384.
- Similar meanings → vectors close together.

This is what `all-MiniLM-L6-v2` (via `SentenceTransformer`) does.

---

## 4. What breaks text into chunks, what converts to tokens, what does pooling?

### Q: In our project, *what exactly* is doing chunking, tokenization, and pooling?

**Chunking (text → chunks):**
- Implemented in your Python code in `src/rag_windows.py` (a helper like `chunk_text`).
- Uses pure string/word logic:
  - Split text into words.
  - Group into ~250‑word chunks with ~50‑word overlap.

**Tokenization (chunks → token IDs):**
- Done by the **tokenizer inside `SentenceTransformers`** (Hugging Face tokenizer loaded with `all-MiniLM-L6-v2`).
- Breaks strings into sub‑word tokens and maps to IDs.

**Pooling (token vectors → one sentence vector):**
- Done by the **`SentenceTransformer` model** itself.
- After MiniLM produces per‑token vectors, the framework applies **mean pooling** to get one 384‑D vector per chunk.

---

## 5. Who converts the query into embeddings?

### Q: When I ask a question (query), who turns that into a vector?

**Answer:**
The **exact same embedding model** used for documents: `all-MiniLM-L6-v2` via `SentenceTransformer`.

- During query:
  1. We call `model.encode([query_string], normalize_embeddings=True, convert_to_numpy=True)`.
  2. That runs the same tokenization → transformer → pooling → normalization pipeline.
  3. Output is a single 384‑dim vector for the query.
- This ensures queries and documents live in the *same* vector space, enabling meaningful cosine similarity.

---

## 6. Where do the two models come from, and are they APIs?

### Q: These two models (embedding + generation) – do we call some remote API, or are they local?

**Answer:**
Both are **local** models; no external API is used at runtime.

- **Embedding model (`all-MiniLM-L6-v2`):**
  - Provided by the `sentence-transformers` Python library.
  - On *first use*, the library downloads model weights from Hugging Face and caches them locally.
  - Thereafter, it just loads them from disk (no network).

- **Generation model (`model.gguf`, e.g., TinyLlama):**
  - You manually downloaded the `.gguf` file and saved it to `models/model.gguf`.
  - Loaded and run via `llama-cpp-python` (Llama.cpp bindings) on CPU.

So:
- **No OpenAI/remote APIs**.
- Embeddings: `SentenceTransformers` + PyTorch.
- Generation: `llama-cpp-python` + GGUF weights.

---

## 7. What does “first time running the code” mean for model download?

### Q: You said "first time it’s fetched from HF" – what exactly happens then?

**Answer:**
For `all-MiniLM-L6-v2`:

- When you call:
  ```python
  from sentence_transformers import SentenceTransformer
  model = SentenceTransformer("all-MiniLM-L6-v2")
  ```
- Internally:
  1. `SentenceTransformer` checks a local cache directory (e.g. `C:\Users\<you>\.cache\torch\sentence_transformers\all-MiniLM-L6-v2`).
  2. **If the model is not there**, it downloads config, tokenizer, and weight files (`pytorch_model.bin`, etc.) from Hugging Face.
  3. Saves them in that cache directory.
  4. Loads them into memory (PyTorch model + tokenizer).

- On all **subsequent runs**:
  - It just loads from that cache on disk – no download, no network required.

So "first time" = first time on a given machine where the cache is empty.

---

## 8. How is `all-MiniLM-L6-v2` actually used after it’s cached?

### Q: I get that it’s downloaded once then cached. After that, how is it used internally?

**Answer (step-by-step after cache exists):**

1. **Load model from disk into RAM**
   - `SentenceTransformer("all-MiniLM-L6-v2")` reads:
     - `config.json` – model architecture.
     - `pytorch_model.bin` – millions of learned weights.
     - tokenizer config/vocab.
   - Builds:
     - A HuggingFace `AutoModel` (MiniLM encoder) as a `torch.nn.Module`.
     - A tokenizer object.
     - A pooling module (mean pooling).

2. **Encode documents or queries** via `model.encode(texts, ...)`

   Under the hood, for each batch of texts:

   - **Tokenization:**
     ```python
     encoded = tokenizer(
         texts,
         padding=True,
         truncation=True,
         return_tensors="pt",
     )
     ```
     Produces `input_ids` and `attention_mask` tensors.

   - **Transformer forward pass:**
     ```python
     with torch.no_grad():
         outputs = encoder(
             input_ids=encoded["input_ids"],
             attention_mask=encoded["attention_mask"],
         )
     ```
     Gives `last_hidden_state` of shape `(batch_size, seq_len, 384)`.

   - **Pooling (sentence embedding):**
     ```python
     token_embeddings = outputs.last_hidden_state
     mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
     sum_embeddings = (token_embeddings * mask_expanded).sum(1)
     sum_mask = mask_expanded.sum(1)
     sentence_embeddings = sum_embeddings / sum_mask  # shape: (batch_size, 384)
     ```

   - **L2 Normalization (optional but used here):**
     ```python
     sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)
     ```

   - **Conversion to NumPy:**
     ```python
     vectors = sentence_embeddings.cpu().numpy().astype("float32")
     ```

3. **Use those vectors:**
   - During **index build**, they go into FAISS (`index.add(vectors)`).
   - During **query**, one vector goes into FAISS search (`index.search(q_vec, top_k)`).

Everything after the first download is purely local: disk → RAM → CPU math.

---

## 9. Embedding vs Generation – tools and responsibilities

### Q: Which model/library does what, in one table?

**Answer:**

| Stage               | What happens                             | Model                          | Library                 | Runs where? |
|---------------------|------------------------------------------|--------------------------------|-------------------------|-------------|
| Chunking            | Long text → overlapping chunks           | (pure Python)                  | your `rag_windows.py`   | CPU         |
| Embedding (docs)    | Chunks → 384‑D vectors                   | `all-MiniLM-L6-v2`             | `sentence-transformers` | CPU         |
| Embedding (queries) | Query → 384‑D vector                     | `all-MiniLM-L6-v2`             | `sentence-transformers` | CPU         |
| Vector index        | Store/search vectors                     | –                              | `faiss-cpu`             | CPU         |
| Generation          | Context + question → answer text         | TinyLlama / other `.gguf` LLM  | `llama-cpp-python`      | CPU         |

This separation is the heart of RAG: **small, fast embedder for search; bigger LLM for answering.**

---

You can extend this document any time with new Q&A as you explore more of the system.